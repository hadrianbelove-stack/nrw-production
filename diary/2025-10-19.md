# DAILY_CONTEXT.md
**Date:** 2025-10-19
**Previous diary entry:** diary/2025-10-17.md

---

## AI Assistant Quick Start

**READ THESE FILES FIRST WHEN STARTING A NEW SESSION:**

1. **This file (DAILY_CONTEXT.md)** - Current state, recent changes, active issues
2. **[PROJECT_CHARTER.md](PROJECT_CHARTER.md)** - Governance rules, amendments, API keys, architectural decisions
3. **[NRW_DATA_WORKFLOW_EXPLAINED.md](NRW_DATA_WORKFLOW_EXPLAINED.md)** - Data pipeline mechanics, how everything fits together

**What is this rolling context system?**

This is a **living document** that gets overwritten each session with current information. At the end of each session, we archive it to `diary/YYYY-MM-DD.md` (immutable historical record). This approach:
- **Avoids token waste** from loading months of PROJECT_LOG.md history
- **Provides fresh context** without stale information
- **Maintains audit trail** in the diary/ folder
- **Reduces AI confusion** by keeping focus on current state

See [AMENDMENT-036](PROJECT_CHARTER.md#amendment-036-rolling-daily-context) for governance rules.

---

## Current State

### What's Working
- GitHub Actions automation: ✅ Operational (runs daily at 9 AM UTC)
- Data pipeline: ✅ Production-ready (1,184 movies tracked, 235 on display as of Oct 18)
- Launch workflow: ✅ One-command startup via `./launch_NRW.sh`
- **NEW:** Watch links integration: ✅ Watchmode API operational with deep links (agent scraper disabled due to authentication barriers)

### Architecture
- Runtime: `index.html` → `assets/app.js` + `assets/styles.css` → `data.json`
- Generation: `movie_tracking.json` → `generate_data.py` → `data.json` (235 movies)
- **NEW:** Watch links: Watchmode API → `cache/watch_links_cache.json` → `data.json.watch_links`
- Automation: GitHub Actions → `daily_orchestrator.py` → pipeline → auto-commit

---

## What We Did Today (2025-10-19)

### Agent Scraper Diagnostic Testing & Resolution
- **Created diagnostic infrastructure:**
  - Created `cache/screenshots/.gitkeep` to ensure directory is tracked in git
  - Ran agent scraper diagnostic test in visible mode (without --headless flag)
  - Captured failure screenshots showing actual HTML structure vs. expected selectors
  - Analyzed 70+ cache entries from Oct 17 - confirmed 100% failure rate
  - Documented findings in `AGENT_SCRAPER_DIAGNOSTICS.md`

- **Root cause identified:**
  - CSS selectors are NOT outdated - they're correct
  - Netflix shows login/verification wall blocking search results
  - Anti-bot detection triggered (reCAPTCHA loaded, verification code required)
  - All platforms require authentication to access search results
  - Impact: 100% failure rate across Netflix, Disney+, Hulu, HBO Max

- **Resolution implemented:**
  - Disabled agent scraper in config.yaml (set enabled: false)
  - Fixed duplicate agent_scraper section in config.yaml (lines 42-49 removed)
  - Watchmode API confirmed as primary source for watch links
  - No functionality loss - Watchmode provides same data without authentication barriers
  - Created AGENT_SCRAPER_FUTURE_IMPLEMENTATION.md documenting Path B for future reference

### Authentication Incident Resolution
- **Authentication error on Oct 17:**
  - OAuth token expired during end-of-session workflow
  - Error: "API Error: 401 authentication_error - OAuth token has expired"
  - User manually committed changes via "commit changes" button
  - Extensions restarted successfully
  - Result: Oct 17 work WAS committed (commit `0d7f978`), but diary creation interrupted

### Documentation Reconstruction
- **What we did:**
  - Reviewed git status and verified no uncommitted changes remain
  - Analyzed git log to confirm Oct 17 work was successfully committed
  - Verified all files documented in DAILY_CONTEXT.md exist in repository
  - Confirmed agent scraper files are in place but returning null links
  - Created missing `diary/2025-10-17.md` retroactively
  - Updated DAILY_CONTEXT.md to reflect accurate Oct 19 state
  - Documented authentication incident and recovery procedure

### Configuration Verification & Cleanup (Oct 19)
- **Verified config.yaml structure:**
  - Confirmed duplicate `agent_scraper` section already removed (only one section exists at lines 20-32)
  - Verified YAML structure is valid and loads correctly
  - Agent scraper properly disabled with explanatory comment

- **Verified .gitignore configuration:**
  - Confirmed `.gitignore` lines 7-9 have correct pattern for cache directory
  - Line 7: `cache/` excludes cache contents
  - Line 8: `!cache/.gitkeep` exception to track placeholder
  - Line 9: `!cache/screenshots/.gitkeep` exception to track screenshots placeholder

- **Fixed missing .gitkeep files:**
  - Created `cache/.gitkeep` to ensure cache directory is tracked in git
  - Created `cache/screenshots/.gitkeep` to ensure screenshots subdirectory is tracked
  - Prevents "cache directory lost between git operations" issue

### Authentication Incident Resolution
- **OAuth Token Expiration (Oct 17):**
  - Occurred during end-of-session workflow
  - Prevented normal Claude Code commit and diary creation
  - User manually committed via UI button (successful)
  - This session reconstructed missing documentation
  - Context integrity now restored

---

## Conversation Context (Key Decisions)

### Decision: Watchmode API vs Streaming Availability API
- **Problem:** Need direct deep links to streaming platforms, not Google searches
- **Options evaluated:**
  1. Streaming Availability API (RapidAPI) - 100 requests/day free tier
  2. Watchmode API - 1,000 requests/month free tier
  3. JustWatch URL construction - No API, but unreliable slug guessing
  4. Manual overrides only - Not scalable for 235 movies

- **Testing results:**
  - Streaming Availability API: ❌ No data for October 2025 releases
  - Watchmode API: ✅ 6 sources for "The Long Walk" (Oct 2025), 115 sources for "Dune: Part Two"

- **Decision:** Watchmode API chosen for better new release coverage
- **Rationale:** 10x more free requests (1,000/month vs 100/day), better October 2025 coverage, real deep links

### Decision: Schema Structure (streaming/rent/buy/default)
- **Canonical categories:** `streaming` (subscription), `rent` (rental), `buy` (purchase), `default` (fallback)
- **Why not free/paid:** More semantic clarity - users understand "streaming" vs "rent" better than "free" vs "paid"
- **Migration support:** Legacy cache format (`free/paid`) automatically migrated to new schema
- **Implementation:** Each category contains `{service: string, link: string}` object

### Decision: Three-Tier Fallback System
- **Tier 1:** Watchmode API deep links (best UX - direct to movie page)
- **Tier 2:** Platform-specific search URLs (good UX - direct to platform's search)
- **Tier 3:** Amazon video search (acceptable UX - universal fallback)
- **Rationale:** Graceful degradation ensures every movie has a working link, even if not perfect

### Decision: Service Priority Hierarchies
- **Streaming:** Netflix > Disney+ > HBO Max > Hulu > Amazon Prime > MUBI
- **Paid:** Amazon Video > Apple TV > Vudu > Google Play
- **Rationale:** Prioritizes platforms with largest user bases (higher chance user has subscription)
- **Example:** If movie is on both Netflix and MUBI, show Netflix (more ubiquitous)

---

## Known Issues

### Issue: Watchmode API Coverage Gaps
- **Symptom:** Some October 2025 movies have no Watchmode data (e.g., "The Roughneck")
- **Root cause:** API has lag time for very new releases
- **Mitigation:** Fallback to platform-specific search URLs (Apple TV, Amazon)
- **Impact:** Some movies use search links instead of deep links (acceptable UX)
- **Status:** Accepted limitation; coverage improves over time

### Issue: Platform-Specific Link Limitations
- **Symptom:** Can't build direct links for Netflix, Disney+, HBO Max
- **Root cause:** These platforms don't have predictable URL patterns (need internal IDs)
- **Mitigation:** Return null for these platforms, frontend uses Google search fallback
- **Impact:** Streaming services without Watchmode data use search links
- **Future:** Agent-based link finding could scrape actual URLs (optional enhancement)

### Issue: Agent Scraper 100% Failure Rate (✅ RESOLVED - Oct 19)
- **Symptom:** Agent scraper executed 70+ times on Oct 17 with 100% failure rate
- **Root causes (diagnosed Oct 19):**
  - **Primary cause:** Authentication walls blocking access (Netflix shows login/verification page)
  - **Secondary cause:** Anti-bot detection (reCAPTCHA, verification codes required)
  - **Not the issue:** CSS selectors are correct, platforms require login first
- **Resolution:**
  - ✅ Agent scraper disabled in config.yaml
  - ✅ Watchmode API is primary source for watch links (already integrated and operational)
  - ✅ No functionality loss - Watchmode provides same data without authentication barriers
- **Reference:** See `AGENT_SCRAPER_DIAGNOSTICS.md` for full analysis

### Issue: Authentication Token Expiration (Oct 17 - RESOLVED)
- **Symptom:** OAuth token expired during end-of-session workflow
- **Root cause:** Token lifetime exceeded during session
- **Impact:** Unable to complete normal end-of-session workflow
- **Resolution:** User manually committed via "commit changes" button
- **Status:** ✅ RESOLVED - work was committed, documentation reconstructed
- **Prevention:** Refresh tokens proactively before long sessions

### Issue: Daily Automation Merge Conflicts (RESOLVED - Oct 17)
- **Symptom:** User gets merge conflicts every morning when pulling from GitHub
- **Root cause:** Bot and user both commit to main branch, causing conflicts in data.json
- **Solution:** Separate branch strategy (AMENDMENT-043)
  - Bot pushes to automation-updates branch (force-push, always succeeds)
  - User merges when ready via ./sync_daily_updates.sh
  - Weekly full regen ensures all movies get retroactive improvements
- **Status:** ✅ Resolved via separate branch architecture
- **Testing:** ⏳ Verify automation-updates branch strategy works in production

### Issue: Cache Migration from Legacy Format
- **Symptom:** Early testing used `free/paid` categories instead of `streaming/rent/buy`
- **Solution:** Automatic migration in `_migrate_legacy_cache_format()` (lines 407-424)
- **Impact:** Old cache entries are automatically converted to new schema
- **Status:** ✅ Resolved via migration function

---

## Next Priorities

### Completed (Oct 16-17)
- ✅ Integrate Watchmode API into `generate_data.py` (AMENDMENT-038)
- ✅ Implement three-button UI (STREAM/RENT/BUY) in `assets/app.js`
- ✅ Add agent-based link finding for Netflix/Disney+/HBO Max/Hulu (AMENDMENT-039)
- ✅ Migrate agent scraper to Playwright (AMENDMENT-041)
- ✅ Inline RT scraper into generate_data.py (AMENDMENT-042)
- ✅ Fix admin panel integration (file paths, 20-movie limit, date update)
- ✅ Add watch link override system to admin panel
- ✅ Add HTTP authentication to admin panel
- ✅ Archive redundant scrapers (wikidata, reelgood, date_verification)
- ✅ Update documentation (NRW_DATA_WORKFLOW_EXPLAINED.md, museum_legacy/README.md)
- ✅ Formalize streaming/rent/buy schema in PROJECT_CHARTER.md (canonical schema section)
- ✅ Add schema validation function to generate_data.py (validate_watch_links_schema)
- ✅ Verify schema consistency across codebase (generate_data.py, app.js, admin.py, caches)

### Completed (Oct 17-19)
- ✅ Verify no uncommitted changes remain from authentication incident
- ✅ Reconstruct missing diary entry for Oct 17
- ✅ Document authentication incident and recovery procedure
- ✅ Verify all documented files exist in repository

### Completed (Oct 19 - Diagnostics & Resolution)
- ✅ Created `cache/screenshots/.gitkeep` to ensure directory is tracked
- ✅ Tested agent scraper with `python3 test_agent_scraper.py`
- ✅ Captured and analyzed failure screenshots
- ✅ Documented findings in `AGENT_SCRAPER_DIAGNOSTICS.md`
- ✅ Identified root cause: authentication walls, not selector issues
- ✅ Disabled agent scraper in config.yaml
- ✅ Fixed duplicate agent_scraper section in config.yaml
- ✅ Confirmed Watchmode API as primary source
- ✅ Verified config.yaml has no duplicate sections (already cleaned up)
- ✅ Verified .gitignore has correct exceptions for .gitkeep files
- ✅ Created missing cache/.gitkeep file
- ✅ Created missing cache/screenshots/.gitkeep file
- ✅ Tested config.yaml loads correctly without errors

### Immediate (Testing & Verification)
- ⏳ Run full regeneration with agent scraper disabled to verify Watchmode API coverage: `python3 generate_data.py --full --debug`
- ⏳ Monitor Watchmode API usage and rate limits during daily automation
- ⏳ Test admin panel watch link override functionality
- ⏳ Verify GitHub Actions workflow succeeds with Playwright installation
- ⏳ Test daily automation with automation-updates branch strategy
- ⏳ Test weekly full regeneration workflow
- ⏳ Test sync_daily_updates.sh script
- ⏳ Verify data quality validation catches bad data
- ⏳ Verify GitHub issue creation on failures

### Short-term (Next Session)
- Monitor Watchmode API coverage and identify any gaps
- Consider migrating RT scraper to Playwright (optional performance improvement)
- Consider migrating YouTube scraper to Playwright (optional consistency improvement)
- If Watchmode API becomes insufficient:
  - See `AGENT_SCRAPER_FUTURE_IMPLEMENTATION.md` for authentication implementation plan
  - Would require login automation, stealth plugins, and session management

### Long-term (Ongoing Maintenance)
- Update scraper selectors when platforms change HTML (every 3-6 months)
- Monitor cache hit rates and scraper statistics
- Review screenshot diagnostics for recurring failures
- Consider parallel scraping for performance (if dataset grows significantly)

---

## Archive Instructions

**End-of-session workflow (automated via `ops/archive_daily_context.sh`):**

1. Run archive script: `./ops/archive_daily_context.sh`
   - Archives current context to `diary/YYYY-MM-DD.md`
   - Creates fresh template for next session
   - Use `--dry-run` to preview changes without executing

2. **Testing:** `./ops/archive_daily_context.sh --dry-run` shows what would happen

3. **Troubleshooting:**
   - Permission error: `chmod +x ops/archive_daily_context.sh`
   - Missing file error: Ensure you're in repo root
   - Directory issues: Script creates `diary/` automatically

4. **Next session starts fresh:**
   - AI reads new DAILY_CONTEXT.md template
   - Historical context available in `diary/` if needed
   - No token waste from stale information

**Current status:** Archive script created and ready to use

---

## Files Changed Today

### Created (Oct 19 - Diagnostics)
- `cache/screenshots/.gitkeep` - Ensures screenshot directory is tracked in git
- `AGENT_SCRAPER_DIAGNOSTICS.md` - Comprehensive agent scraper diagnostic report with test results and recommendations
- `cache/.gitkeep` - Ensures cache directory is tracked in git (prevents directory loss during git operations)

### Modified (Oct 19 - Documentation & Resolution)
- `config.yaml` - Removed duplicate agent_scraper section, disabled agent scraper
- `AGENT_SCRAPER_DIAGNOSTICS.md` - Added decision and testing plan
- `DAILY_CONTEXT.md` - Updated with resolution
- `diary/2025-10-17.md` - Created retroactively (was interrupted by authentication error)
- `diary/2025-10-19.md` - Created for this diagnostic session
- `PROJECT_CHARTER.md` - Added AMENDMENT-044 for authentication token management

### Created (Oct 17 - From interrupted session)
- `cache/watch_links_cache.json` - Watchmode API response cache (228 entries)
- `cache/.gitkeep` - Ensures cache directory is tracked in git
- `test_agent_scraper.py` - Standalone test for agent scraper debugging
- `test_rt_scraper_inline.py` - Standalone test for inlined RT scraper
- `cache/screenshots/.gitkeep` - Ensures screenshot directory is tracked in git
- `.github/workflows/weekly-full-regen.yml` - Sunday full regeneration workflow
- `sync_daily_updates.sh` - User script to merge automation data

### Modified
- `generate_data.py` - Added Watchmode API integration (lines 197-400), `argparse` support (lines 14, 640-649)
- `data.json` - Regenerated with `watch_links` field (235 movies)
- `assets/app.js` - Added WATCH button to card backs (line 130), updated watch link logic (lines 84-101)
- `assets/styles.css` - Reduced WATCH button height (padding: 0.5rem)
- `index.html` - Added cache-busting parameters (?v=3 for CSS, ?v=2 for JS)
- `PROJECT_CHARTER.md` - Added AMENDMENT-038 for Watchmode API integration, added Watchmode API key to API Keys section, added AMENDMENT-043 documenting bulletproof daily automation
- `DAILY_CONTEXT.md` - This file (documented watch links feature completion)
- `requirements.txt` - Added selenium, webdriver-manager, beautifulsoup4, lxml
- `generate_data.py` - Enhanced debug logging, config reading for agent_scraper section
- `agent_link_scraper.py` - Added comprehensive debug logging throughout
- `admin.py` - Fixed file paths (admin/ not output/), removed 20-movie limit, added watch link override UI, added regenerate button, added HTTP auth
- `config.yaml` - Added agent_scraper and rt_scraper configuration sections
- `daily_orchestrator.py` - Removed date_verification.py and update_rt_data.py from pipeline, added validate_data_quality() method with comprehensive checks
- `daily_update.sh` - Removed date_verification.py and update_rt_data.py calls
- `.github/workflows/daily-check.yml` - Added Playwright browser installation, updated to push to automation-updates branch with force-push, added data quality validation, added failure notifications
- `NRW_DATA_WORKFLOW_EXPLAINED.md` - Updated scraper architecture documentation
- `museum_legacy/README.md` - Added scripts/rt_scraper.py to archived list
- `PROJECT_CHARTER.md` - Added "Canonical Watch Links Schema" section, updated AMENDMENT-031 to remove unused `default` category, added schema validation reference to AMENDMENT-038
- `generate_data.py` - Added validate_watch_links_schema() function with comprehensive validation and statistics tracking

### Archived (Oct 17, 2025)

**Scrapers moved to museum_legacy/:**
- `wikidata_scraper.py` - Redundant with Wikipedia REST API
- `reelgood_scraper.py` - Redundant with TMDB API
- `date_verification.py` - Only user of reelgood_scraper (non-critical)
- `rt_scraper.py` (root) - Old RT scraper version (v1)
- `scripts/rt_scraper.py` - Newer RT scraper (v2), inlined into generate_data.py (v3)
- `update_rt_data.py` - RT scraping now automatic
- `bootstrap_rt_cache.py` - RT cache built automatically

**Admin tools deleted:**
- `museum_legacy/curator_admin.py` - Orphaned admin UI (expected non-existent files)
- `museum_legacy/run_admin_5100.py` - Launcher for curator_admin.py

**See:** `museum_legacy/README.md` for detailed archival documentation and migration notes

---

## Quick Reference

### Daily Workflow
```bash
# Morning: View the wall
./launch_NRW.sh

# Automation runs automatically at 9 AM UTC
# (No manual intervention needed)
```

### Sync Automation Data
```bash
# Merge automation updates from bot
./sync_daily_updates.sh

# What it does:
# 1. Fetches automation-updates branch
# 2. Shows what changed
# 3. Merges into main
# 4. Shows latest movies
```

### Automation Schedule
- **Daily:** 9 AM UTC (2 AM PDT) - Incremental update (new movies only)
- **Weekly:** Sunday 10 AM UTC (2 AM PDT) - Full regeneration (all movies)
- **User sync:** Run `./sync_daily_updates.sh` after automation completes

### Manual Pipeline (if needed)
```bash
# Check for new digital releases
python3 movie_tracker.py check

# Regenerate data.json
python3 generate_data.py

# Add new theatrical releases (weekly)
python3 movie_tracker.py bootstrap
```

### Context Files (Read These First)
- **Daily:** This file (DAILY_CONTEXT.md) - Current state and recent changes
- **Governance:** [PROJECT_CHARTER.md](PROJECT_CHARTER.md) - Rules, amendments, API keys
- **Pipeline:** [NRW_DATA_WORKFLOW_EXPLAINED.md](NRW_DATA_WORKFLOW_EXPLAINED.md) - How data flows
- **History:** `diary/YYYY-MM-DD.md` - End-of-session archives (when needed)

---

**Last updated:** 2025-10-19 21:14 (Session end - ready for archive)
**Next diary archive:** End of session -> `diary/2025-10-19.md`